# 逆战爬虫第二天

* 复习   
* urllib案例    
* requests库   
  * 案例



## 复习 

```python
urllib.request.urlopen() 请求url返回对象 但是不能构建header头信息 也就是不能添加User-Agent信息  
urllib.request.urlretrieve() 也不能添加header头信息
于是 我们需要引入Request对象可以自己定制请求头 


html = res.read().decode('gbk') #bytes 类型-> str类型  


```



## 抓包工具的使用 

```txt
打开软件：
1、配置软件，配置fiddler能够抓取https的包
	Tools==>Options==>HTTPS
	选中 Capture Https Connects 
	选中 Decrypt Https Traffic
	选中 Ignore
	然后将fiddler关闭再次打开即可
2、fiddler软件介绍
	左边栏、右边栏
	左边栏：所有的请求
		html   <>
		css    图片中的标记
		js     前面标注有js
		json   前面标注有json
		post   一个书本，一个箭头
	右边栏：点击左边栏其中一个请求，这个请求的详细信息就会显示到右边栏
		右上边栏：http请求信息
			点击  Insepctors
			webforms：post请求所有的表单数据
			raw：整个请求以纯文本的格式显示给你
		右下边栏：http请求响应信息
			有一个黄色的提示信息：响应体被编码过，需要点击进行解码，然后点击即可
			headers：响应头信息
			textview：响应的信息以文本的形式显示出来
			imageview：如果图片，在这里显示图片
			webview：模拟浏览器显示
			cookies：cookie信息
			raw：将响应的信息以纯文本的形式展示给你
			json：一些接口返回给你json，在这里查看
3、禁止fiddler抓包，file，点击第一个选项取消对号即可
4、清楚所有的请求， 点击x号，remove all
5、左下角黑色框框，输入指令的地方
	select json
	select html
	select image
	cls 清楚所有请求
	?关键词  搜索
```



## 爬虫 与反爬  、反反爬 

> 最终胜利的 是爬虫  

* get 转成post 
* user-agent 
* ip地址  
* js混淆加密  
* 自己加密 
* css 偏移 
* 图形验证码  



## 案例 

### 51job

```python
from urllib import request
import re
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

url = "https://search.51job.com/list/180200%252C190200%252C020000%252C040000%252C080200,000000,0000,00,9,99,python,2,1.html?lang=c&stype=&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&providesalary=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=&welfare="

req = request.Request(url,headers=headers)

res = request.urlopen(req)


html = res.read().decode('gbk')

#处理数据
job_num_re = '<div class="rt">(.*?)</div>'
comp = re.compile(job_num_re,re.S)
jobnum_str = comp.findall(html)[0]
# print(jobnum_str) # 共14080条职位 及空格

#提取数字
num_re = ".*?(\d+).*?"
num = re.findall(num_re,jobnum_str)[0]
# print(int(num))


#获取第一个岗位名称
joblist_re = '<div class="el">(.*?)</div>'
joblist_str = re.findall(joblist_re,html,re.S)
# print(joblist_str[0])
# print(joblist_str[0])

# joblist_re1 = 'onmousedown="">(.*?)</a>'
# joblist_str1= re.findall(joblist_re1,joblist_str[0],re.S)
# print(joblist_str1)

for job in joblist_str:
    joblist_re1 = 'onmousedown="">(.*?)</a>'
    try:
        joblist_str1 = re.findall(joblist_re1,job,re.S)
        print("岗位名称:",joblist_str1[0].strip())
    except:
        pass
```



### 豆瓣电影

```python
from urllib import request
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}
for i in range(10):
    url = "https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start="+"%s" %(i*20)
    req = request.Request(url,headers=headers)
    response = request.urlopen(req)
    content = response.read().decode()
    # print(content)
        #解析json
    # print(json.loads(content))
    data = json.loads(content)
    data_list = data.get('subjects')
    for movie in data_list:
        title = movie['title']
        url = movie['url']
        print(title,url)
```



### 网易云  

```python
from urllib import request
from urllib import parse
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

url = "https://music.163.com/weapi/v1/resource/comments/A_PL_0_924680166?csrf_token=6789630a244c9d7be267d088f0bbe1d7"


data = {
    "params":"mHGCIYkOHJX/oh5U406M1WxDG2PLmsITXpiO72w/OAcQcDCbgnPdPKQAklWewgUSUvwnI4jUOB+MAya+TP7elRexKUl0+qh0eZpPTCn0GfyvqsrGMIESAia2vK2BvkaQWo8IcKLZ6rLpMeUwmYs7T7/kmSJ+W0KZYk+G9rA49hnL1HO3xk7XqQcsTr7SUD7oQg8GjUs4tF8n90aLorGXpLLYY65XGM9XtncQAbfvnSo=",
    "encSecKey":"31d89f28b1878d7935c4e5c0416438d542102e90273669d2206c837427f2313e1cbf26af2572b736e2adee43bb284bd767e2fd7ce6c9e3a0540f04bd2f863e682c8fa9d46d7e082955de3e66a14649b1bbec6f7f8fab026cc473f116c99c88cf3704ead5763fb565f41a4201f6127d736e0f31bbf68918f69cde49111f411aa0"
}

#将参数进行url编码  变成浏览器是别的
#urlencode 如果url 中 有中文及特殊字符 自动的进行编码
#如果我们使用代码发送请求  那么必须手动进行编码
#用 urlib.parse.urlencode
data  = parse.urlencode(data).encode()
#decode 是 由 bytes类型 => str类型
#encode 是 str类型 => bytes类型
# print(data)

req = request.Request(url,headers=headers,data=data) #传入二进制参数

res = request.urlopen(req)
# print(res.read().decode('utf-8'))
content = res.read().decode('utf-8')

#解析json
data_dict = json.loads(content)
# print(data_dict)

hotComments = data_dict['hotComments']
for hotComment in hotComments:
    nickname = hotComment['user']['nickname']
    content = hotComment['content']
    print(nickname,":",content)
```



### 阿里招聘

```python
from urllib import request
from urllib import parse
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

    # url = "https://job.alibaba.com/zhaopin/socialPositionList/doList.json?pageSize="+"%d"+"&t=0.7576443766726075&keyWord=python" %( i*10)
url = "https://job.alibaba.com/zhaopin/socialPositionList/doList.json"
for i in range(100):
    params = {
        't':"0.7576443766726075",
        'pageSize':i*10,
    }

    data = parse.urlencode(params).encode()
    req = request.Request(url=url,headers=headers,data=data,unverifiable=True)
    response = request.urlopen(req)
    content = response.read().decode()

    # print(content)
    data_dict = json.loads(content)

    joblist = data_dict['returnValue']['datas']
    for job in joblist:
        degree = job.get('degree')
        departmentName = job.get('departmentName')
        workExperience = job.get('workExperience')
        requirement = job.get('requirement')

        with open('ali.txt','a+',encoding='utf-8') as fp:
            fp.write(str((degree,departmentName,workExperience,requirement))+"\n")
            fp.flush() #不按下回车键也能写入数据
```











