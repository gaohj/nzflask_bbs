# 逆战爬虫第二天

* 复习   
* urllib案例    
* requests库   
  * 案例



## 复习 

```python
urllib.request.urlopen() 请求url返回对象 但是不能构建header头信息 也就是不能添加User-Agent信息  
urllib.request.urlretrieve() 也不能添加header头信息
于是 我们需要引入Request对象可以自己定制请求头 


html = res.read().decode('gbk') #bytes 类型-> str类型  


```



## 抓包工具的使用 

```txt
打开软件：
1、配置软件，配置fiddler能够抓取https的包
	Tools==>Options==>HTTPS
	选中 Capture Https Connects 
	选中 Decrypt Https Traffic
	选中 Ignore
	然后将fiddler关闭再次打开即可
2、fiddler软件介绍
	左边栏、右边栏
	左边栏：所有的请求
		html   <>
		css    图片中的标记
		js     前面标注有js
		json   前面标注有json
		post   一个书本，一个箭头
	右边栏：点击左边栏其中一个请求，这个请求的详细信息就会显示到右边栏
		右上边栏：http请求信息
			点击  Insepctors
			webforms：post请求所有的表单数据
			raw：整个请求以纯文本的格式显示给你
		右下边栏：http请求响应信息
			有一个黄色的提示信息：响应体被编码过，需要点击进行解码，然后点击即可
			headers：响应头信息
			textview：响应的信息以文本的形式显示出来
			imageview：如果图片，在这里显示图片
			webview：模拟浏览器显示
			cookies：cookie信息
			raw：将响应的信息以纯文本的形式展示给你
			json：一些接口返回给你json，在这里查看
3、禁止fiddler抓包，file，点击第一个选项取消对号即可
4、清楚所有的请求， 点击x号，remove all
5、左下角黑色框框，输入指令的地方
	select json
	select html
	select image
	cls 清楚所有请求
	?关键词  搜索
```



## 爬虫 与反爬  、反反爬 

> 最终胜利的 是爬虫  

* get 转成post 
* user-agent   http://www.useragentstring.com/pages/useragentstring.php?name=Firefox  
* ip地址  
* js混淆加密  
* 自己加密 
* css 偏移 
* 图形验证码  



## 案例 

### 51job

```python
from urllib import request
import re
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

url = "https://search.51job.com/list/180200%252C190200%252C020000%252C040000%252C080200,000000,0000,00,9,99,python,2,1.html?lang=c&stype=&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&providesalary=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=&welfare="

req = request.Request(url,headers=headers)

res = request.urlopen(req)


html = res.read().decode('gbk')

#处理数据
job_num_re = '<div class="rt">(.*?)</div>'
comp = re.compile(job_num_re,re.S)
jobnum_str = comp.findall(html)[0]
# print(jobnum_str) # 共14080条职位 及空格

#提取数字
num_re = ".*?(\d+).*?"
num = re.findall(num_re,jobnum_str)[0]
# print(int(num))


#获取第一个岗位名称
joblist_re = '<div class="el">(.*?)</div>'
joblist_str = re.findall(joblist_re,html,re.S)
# print(joblist_str[0])
# print(joblist_str[0])

# joblist_re1 = 'onmousedown="">(.*?)</a>'
# joblist_str1= re.findall(joblist_re1,joblist_str[0],re.S)
# print(joblist_str1)

for job in joblist_str:
    joblist_re1 = 'onmousedown="">(.*?)</a>'
    try:
        joblist_str1 = re.findall(joblist_re1,job,re.S)
        print("岗位名称:",joblist_str1[0].strip())
    except:
        pass
```



### 豆瓣电影

```python
from urllib import request
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}
for i in range(10):
    url = "https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start="+"%s" %(i*20)
    req = request.Request(url,headers=headers)
    response = request.urlopen(req)
    content = response.read().decode()
    # print(content)
        #解析json
    # print(json.loads(content))
    data = json.loads(content)
    data_list = data.get('subjects')
    for movie in data_list:
        title = movie['title']
        url = movie['url']
        print(title,url)
```



### 网易云  

```python
from urllib import request
from urllib import parse
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

url = "https://music.163.com/weapi/v1/resource/comments/A_PL_0_924680166?csrf_token=6789630a244c9d7be267d088f0bbe1d7"


data = {
    "params":"mHGCIYkOHJX/oh5U406M1WxDG2PLmsITXpiO72w/OAcQcDCbgnPdPKQAklWewgUSUvwnI4jUOB+MAya+TP7elRexKUl0+qh0eZpPTCn0GfyvqsrGMIESAia2vK2BvkaQWo8IcKLZ6rLpMeUwmYs7T7/kmSJ+W0KZYk+G9rA49hnL1HO3xk7XqQcsTr7SUD7oQg8GjUs4tF8n90aLorGXpLLYY65XGM9XtncQAbfvnSo=",
    "encSecKey":"31d89f28b1878d7935c4e5c0416438d542102e90273669d2206c837427f2313e1cbf26af2572b736e2adee43bb284bd767e2fd7ce6c9e3a0540f04bd2f863e682c8fa9d46d7e082955de3e66a14649b1bbec6f7f8fab026cc473f116c99c88cf3704ead5763fb565f41a4201f6127d736e0f31bbf68918f69cde49111f411aa0"
}

#将参数进行url编码  变成浏览器是别的
#urlencode 如果url 中 有中文及特殊字符 自动的进行编码
#如果我们使用代码发送请求  那么必须手动进行编码
#用 urlib.parse.urlencode
data  = parse.urlencode(data).encode()
#decode 是 由 bytes类型 => str类型
#encode 是 str类型 => bytes类型
# print(data)

req = request.Request(url,headers=headers,data=data) #传入二进制参数

res = request.urlopen(req)
# print(res.read().decode('utf-8'))
content = res.read().decode('utf-8')

#解析json
data_dict = json.loads(content)
# print(data_dict)

hotComments = data_dict['hotComments']
for hotComment in hotComments:
    nickname = hotComment['user']['nickname']
    content = hotComment['content']
    print(nickname,":",content)
```



### 阿里招聘

```python
from urllib import request
from urllib import parse
import json
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}

    # url = "https://job.alibaba.com/zhaopin/socialPositionList/doList.json?pageSize="+"%d"+"&t=0.7576443766726075&keyWord=python" %( i*10)
url = "https://job.alibaba.com/zhaopin/socialPositionList/doList.json"
for i in range(100):
    params = {
        't':"0.7576443766726075",
        'pageSize':i*10,
    }

    data = parse.urlencode(params).encode()
    req = request.Request(url=url,headers=headers,data=data,unverifiable=True)
    response = request.urlopen(req)
    content = response.read().decode()

    # print(content)
    data_dict = json.loads(content)

    joblist = data_dict['returnValue']['datas']
    for job in joblist:
        degree = job.get('degree')
        departmentName = job.get('departmentName')
        workExperience = job.get('workExperience')
        requirement = job.get('requirement')

        with open('ali.txt','a+',encoding='utf-8') as fp:
            fp.write(str((degree,departmentName,workExperience,requirement))+"\n")
            fp.flush() #不按下回车键也能写入数据
```



### urlopen 函数  

> 不能携带header头 需要引入Request对象 所有和网络请求相关的方法 都封装在了  urllib.request下了  

1. `url` 请求的url 
2. `data` 设置这个值 就变成post请求  
3. 返回值 HttpResponse对象  read readline readlines 

### urlretrieve 

```
将 页面、图片保存到本地的方法  不能携带header头 
urllib.request.urlretrieve('url','名字等')
```



### urlencode  

> 将字典转成url编码的数据  
>
> ```
> #将参数进行url编码  变成浏览器识别的 
> #urlencode 如果url 中 有中文及特殊字符 自动的进行编码
> #如果我们使用代码发送请求  那么必须手动进行编码
> ```

```
urllib.parse.urlencode({'username':'kangbazi'}) 
```



### parse_qs 

> 将编码后的url参数进行解码  

```
urllib.parse.parse_qs({}) 
```



### urlparse\urlsplit   

>  对url 进行 切割  两个方法 基本一样  urlsplit    少了一个params参数   

```python
# https://www.baidu.com/s?wd=朱道布鲁克&rsv_spt=1&rsv_iqid=0
url = "https://www.baidu.com/s?wd=朱道布鲁克&rsv_spt=1&rsv_iqid=0"
result = parse.urlparse(url=url)
print(result)

result1 = parse.urlsplit(url=url)
#split
print(result1)  #
print(result1.query)
print(result1.netloc)

SplitResult(scheme='https', netloc='www.baidu.com', path='/s', query='wd=朱道布鲁克&rsv_spt=1&rsv_iqid=0', fragment='')
wd=朱道布鲁克&rsv_spt=1&rsv_iqid=0
www.baidu.com
```



### 反反爬 2  代理  

* 西刺 
* 快代理 
* 代理云 
* 芝麻代理
* 西瓜代理

####　构建处理器　　

* urlopen() 不能构建请求头  于是引入Request
* Request对象能够加请求头 但是 不能携带cookie 信息 及使用代理 需要手动创建处理器   自定义打开器   由打开器代理原来的 urlopen()  

```python
from urllib import request
url = 'http://www.httpbin.org/ip'
res = request.urlopen(url)
print(res.read().decode('utf-8'))



url1 = 'http://www.httpbin.org/ip'
#构建处理器
handler = request.ProxyHandler({'https':'221.224.136.211:35101'})
#创建打开器
opener = request.build_opener(handler)
#构建请求对象
req1 = request.Request(url)
#使用打开器 发送请求
res1 = opener.open(req1)
print(res1.read().decode('utf-8'))

```



### 模拟登录  

```python
from urllib import request

headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
    "Cookie":"anonymid=k7lk0d66y5mtud; depovince=NMG; jebecookies=15f53123-f0c7-4def-89b5-0fa3944825fb|||||; _r01_=1; ick_login=aec74675-5f38-424e-88ec-16d2089c5704; taihe_bi_sdk_uid=37f9cd36a45e1197451328e2b036a7d7; taihe_bi_sdk_session=043baa58c0f2f710027f7d1f89b9a438; _de=28A69782AB906D4A677B8FA35C706602; p=ea113d9ab092b22c38ed46c3931bf75b3; first_login_flag=1; ln_uact=gaohj5@163.com; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=975c7ed8c413cffa8ee74d1eb77016123; societyguester=975c7ed8c413cffa8ee74d1eb77016123; id=541197383; xnsid=3cb8e965; ver=7.0; loginfrom=null; JSESSIONID=abcXF-niHqo3jd54Xrddx; jebe_key=7ce76353-4541-48eb-bb60-d8d79054b7a0%7C67510b08b897c5e3b50e55d3a235a53c%7C1583824184828%7C1%7C1583824184050; jebe_key=7ce76353-4541-48eb-bb60-d8d79054b7a0%7C67510b08b897c5e3b50e55d3a235a53c%7C1583824184828%7C1%7C1583824184054; wp=1; wp_fold=1"
}


# request.urlretrieve('http://www.renren.com/541197383/profile','renren.html')
url = "http://www.renren.com/541197383/profile"
req = request.Request(url,headers=headers)
res = request.urlopen(req)

with open('renren1.html','w',encoding='utf-8') as fp:
    fp.write(res.read().decode('utf-8'))
```



> 以上方式 需要每次访问从浏览器复制cookie信息 有没有存储cookie 的东西   htttp.cookiejar 

```
from http.cookiejar import CookieJar,FileCookieJar,MozillaCookieJar,LWPCookieJar
```

1. CookieJar

   管理http的cookie值 存储请求生成的cookie 整个cookie存在内存中

2. FileCookieJar

   由CookieJar 派生出来 将cookie信息存到文件中

3. MozillaCookieJar

   由FileCookieJar派生而来  用来创建Mozilla 浏览器兼容的FileCookieJar 实例

4. LWPCookieJar

   由FileCookieJar派生而来  用来创建与libwww-perl 兼容的实例 

```python
from http.cookiejar import CookieJar,FileCookieJar,MozillaCookieJar,LWPCookieJar
from urllib import parse
from urllib import request
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}
#创建打开器
def get_opener():
    cookiejar = CookieJar()
    handler = request.HTTPCookieProcessor(cookiejar) #存储cookie信息
    opener = request.build_opener(handler) #携带cookie信息发送请求
    return opener

#登录人人网
def login_renren(opener):
    data = {
        "email":'gaohj5@163.com',
        "password":'12qwaszx',
    }
    data = parse.urlencode(data).encode('utf-8')
    url = "http://www.renren.com/PLogin.do"
    req = request.Request(url=url,headers=headers,data=data)
    opener.open(req)


#访问个人中心  并下载
def visit_profile(opener):
    url = "http://www.renren.com/541197383/profile"
    req = request.Request(url,headers=headers)
    res = opener.open(req)
    with open('renren2.html', 'w', encoding='utf-8') as fp:
        fp.write(res.read().decode('utf-8'))


if __name__ == "__main__":
    opener = get_opener()
    login_renren(opener)
    visit_profile(opener)
```



## requests 库  重点 

> urllib库的升级   
>
> 官方网站https://requests.readthedocs.io/zh_CN/latest/   

```
pip install  requests
```

> 第三方接口 比如短信接口也经常使用 requests 库来发送请求

### 示例

```python
import requests

#GET 请求
response = requests.get('https://www.baidu.com')
print(response) #Response对象
print(response.status_code) #状态码 200
print(response.url) # 请求url地址https://www.baidu.com/
print(response.encoding) #编码类型ISO-8859-1
print(response.cookies) #cookie信息<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>


#get 携带参数
headers = {
    "User-Agent":"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
}
url = 'https://www.douban.com'
params = {
    'wd':'岛国电影'
}
response = requests.get(url=url,headers=headers,params=params)

# print(response.text) #返回的是Unicode格式的数据 根据自己的猜测解码
print(response.content.decode('utf-8')) #返回的是字节流数据 需要手动解码

```



### post请求 

```python
requests.post(url,data)  #data数据字典  

import requests
import json
headers = {
    'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0"
}

url = "http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule"
#_o要去掉
f = input('请输入要翻译的内容:')

#en zh-CHS AUTO
data = {
    "i":f,
    "version":"2.1",
    "ts":"1583828597456",
    "to":" zh-CHS",
    "from":"AUTO",
    "smartresult":"AUTO",
    "client":"fanyideskweb",
    "salt":"15838292461802",
    "sign":"d179ebf9e387480bbed0e48c163147e0",
    "bv":"a9c3483a52d7863608142cc3f302a0ba",
    "doctype":"json",
    "keyfrom":"fanyi.web",
    "action":"FY_BY_CLICKBUTTION",
}

res = requests.post(url,data=data,headers=headers)
# print(res.text)

# dic = json.loads(res.text)
dic = res.json()

tgt = dic['translateResult'][0][0]['tgt']
print(tgt)
```



### 设置代理  

```

```

